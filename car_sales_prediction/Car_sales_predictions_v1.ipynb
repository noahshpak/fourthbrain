{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNnTTMT9dnM4"
   },
   "source": [
    "# Today you are a Data Scientist at Tesla! \n",
    "## You have assigned a new project to look at car sales from Quarters 1-2 in California for 2019 to make predictions as to which cars will be sold more than the others in Q3 and Q4, to ensure enough inventory to meet demands!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQqh5DMaq9QW"
   },
   "source": [
    "### If running this notebook in Google Colab, run the following cells first. Make sure you've placed the Q12 and Q34 sales data in the same Google Drive folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8n75mujnrqU4"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HT1yuyN5zJ19"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6AR4c7ivMtD"
   },
   "source": [
    "Replace the ID with ID of file you want to access. To get this ID: \n",
    "\n",
    "1.   Right-click on the name of the appropriate file in your Google Drive\n",
    "2.   Click \"Get link\"\n",
    "3.   Copy the characters between `d/` and the following slash\n",
    "4.   Paste them into the dictionary argument of the call to `drive.CreateFile()` as the value associated with the 'id' key. Make sure that this value is a string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mb5xkL1QzPYg"
   },
   "outputs": [],
   "source": [
    "# Replace the ID with ID of file you want to access\n",
    "Q12 = drive.CreateFile({'id':'1vpppIqAvvH8O44XEUEYlEk96clxbQEG7'}) \n",
    "Q12.GetContentFile('sales_Q12_2019.csv') \n",
    "Q34 = drive.CreateFile({'id':'1He76yUv5030l_qhugdhSOf3OjUEbrElI'}) \n",
    "Q34.GetContentFile('sales_Q34_2019.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI6NP0JBdnM5"
   },
   "source": [
    "### Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdNx3CHEdnM5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dM4fGT-ldnM8"
   },
   "source": [
    "### Read in the CSV file containing the California sales data for Quarters 1 and 2\n",
    "\n",
    "Then examine the data's shape and first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jj2l01M8dnM9"
   },
   "outputs": [],
   "source": [
    "df_sales = pd.read_csv(\"sales_Q12_2019.csv\")\n",
    "print(df_sales.shape)\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF7w0UQndnM_"
   },
   "source": [
    "### Begin cleaning the data\n",
    "\n",
    "Eliminate the `'dealer_state'` and `'date'` columns. The former is useless to our model, since we already know that our dataset is restricted to California sales. While we could possibly extract useful information from the `'date'` column (for example, to determine whether more cars are sold on weekends than weekdays), we'll be focusing on car configurations in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-DThsrHdnNA"
   },
   "outputs": [],
   "source": [
    "df_sales = df_sales.drop(columns=['dealer_state','date'])\n",
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqdkmnCodnNC"
   },
   "source": [
    "### Read in the CSV file containing the California sales data for Quarters 3 and 4\n",
    "\n",
    "The `'dealer_state'` and `'date'` columns have already been eliminated in this dataset, so you don't need to worry about them here. Examine the data's shape and first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJpTgHxAdnND"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Read in the Q34 data\n",
    "df_pred = None\n",
    "# Print the shape of the data\n",
    "None\n",
    "# Examine the first few rows of the data\n",
    "None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcmsU-T1dnNF"
   },
   "source": [
    "# Task I. Consolidate data by finding numbers of unique car combinations sold for training and test data sets\n",
    "\n",
    "You've probably noticed that 73 of the 74 columns in our Q12 and Q34 datasets are one-hot-encoded representations of the car's `'main_type'`, `'engine'`, and `'sales_version'` values. You've probably also noticed that the final column is the car's `'MSRP'`, or manufacturer's (Tesla's, in this case) suggested retail price. In its current form, the sales data doesn't contain a target. The car's `'main_type'`, `'engine'`, `'sales_version'`, and `'MSRP'` values are all known to us before the time of the sale and do not, by themselves, offer us anything which we can predict. \n",
    "\n",
    "However, each row represents the sale of a single car. If we define a unique car type by its combination of `'main_type'`, `'engine'`, and `'sales_version'` values, the number of rows displaying that combination corresponds to the number of times during that half of the year that that distinct type of car was sold in California. Therefore, we can make training and test datasets where each row now corresponds to a unique car type, and the target value is how many times that car type was sold in California during a given half of the year.\n",
    "\n",
    "## This task requires data wrangling!\n",
    "## Create functions that read the Q12 (df_sales) and Q34 (df_pred) data sets and create train_X, train_Y, test_X and test_Y, respectively. Use pandas and NumPy as needed.\n",
    "\n",
    "train_X, test_X = unique row combinations without MSRP column\n",
    "\n",
    "train_Y, test_Y = number of unique cars sold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRlSIcDGhJ74"
   },
   "outputs": [],
   "source": [
    "def get_features_and_targets(df):\n",
    "    ### START CODE HERE ###\n",
    "    # Create the local data DataFrame as a copy of the input df DataFrame, minus the 'MSRP' column\n",
    "    data = None\n",
    "    # Use the value_counts() method for DataFrames to store the targets as a NumPy array of the \n",
    "    # normalized sales counts associated with each unique combination of 'main_type', 'engine', \n",
    "    # and 'sales_version' values in the data DataFrame\n",
    "    # Make sure the counts are unsorted, so they're listed in order of the combination's first appearance\n",
    "    # in the data DataFrame, and that the array is 2D, with 1 entry per row\n",
    "    Y = None\n",
    "    # Get a list of the column names of the data DataFrame\n",
    "    subset_names = None\n",
    "    # Use the drop_duplicates() method on the df DataFrame to store the features data as a NumPy array where\n",
    "    # each row corresponds to a unique combination of 'main_type', 'engine', and 'sales_version' values\n",
    "    # Make sure to set the subset parameter to subset_names, so the 'MSRP' column isn't included in the \n",
    "    # uniqueness calculations\n",
    "    X = None\n",
    "    # Scale and shift the 'MSRP' column so its values fall in the range [0,1]\n",
    "    # You might find NumPy's ptp() function useful\n",
    "    X[:,-1] = ( X[:,-1] - min(X[:,-1]) ) / np.ptp(X[:,-1])\n",
    "    # Return features and targets\n",
    "    return None\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toTX3W89fX2P"
   },
   "source": [
    "### Extract features and targets as defined above from both the Q12 and Q34 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rf6Vj0ujhJ77"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Extract training features and targets from the Q12 data\n",
    "train_X, train_Y = None\n",
    "# Extract test features and targets from the Q34 data\n",
    "test_X, test_Y = None\n",
    "### END CODE HERE ###\n",
    "print(f\"Number of unique cars in Q12 = {len(train_Y)}\")\n",
    "print(f\"Number of unique cars in Q34 = {len(test_Y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d7BSjsQdnNV"
   },
   "source": [
    "You probably noticed that the Q12 and Q34 datasets contained differing numbers of distinct cars. Clearly, some new models were introduced by Q3, but were any discontinued by the end of Q2? Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luGBEZB5hJ8A"
   },
   "outputs": [],
   "source": [
    "# How many unique cars in 2012 (Q1:Q4)?\n",
    "### START CODE HERE ### \n",
    "# Concatenate the Q12 and Q34 DataFrames into a single DataFrame\n",
    "df_full = pd.concat([df_sales, df_pred])\n",
    "# Extract features and targets from the concatenated Q14 data\n",
    "data_X, data_Y = get_features_and_targets(df_full)\n",
    "### END CODE HERE ###\n",
    "print(f\"Number of unique cars in Q12 and Q34 = {len(data_Y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGEXB0mCdnNb"
   },
   "source": [
    "The following probability equations are true for any two events $A$ and $B$:\n",
    "\n",
    "$$P (A \\cup B) = P(A) + P(B) - P (A \\cap B)$$\n",
    "$$P(A) = P (A \\cap B) + P (A \\cap B^c)$$\n",
    "\n",
    "Use them to determine how many car models were sold in both halves of 2019, how many were discontinued by the second half of the year, and how many were launched in the second half of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hewxl1lMf-VF"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qn6SNBNdnNb"
   },
   "source": [
    "# Task II: Visualize the training and test targets any way you see fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsuuxEqhjD6q"
   },
   "outputs": [],
   "source": [
    "# Visualize the training targets\n",
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyGN9vqLdnNe"
   },
   "outputs": [],
   "source": [
    "# Visualize the test targets\n",
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9g-2fwgydnNh"
   },
   "source": [
    "# Task III: Apply Gradient Descent (Linear, Polynomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXjeuMN3dnNh"
   },
   "source": [
    "Initialize the $\\theta$ parameters as a column vector of zeros, one for every feature in the training data plus one for bias. \n",
    "\n",
    "Also set hyperparameters for learning rate and maximum number of iterations through the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq--V5vSdnNi"
   },
   "outputs": [],
   "source": [
    "# Initialize learned parameters theta and hyperparameters\n",
    "s_theta = np.zeros((train_X.shape[1]+1, 1))\n",
    "s_learning_rate = 0.001\n",
    "s_max_iteration = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx-wS125dnNk"
   },
   "source": [
    "### Hypothesis Function\n",
    "\n",
    "Define your hypothesis function $h$ (which you use to make predictions $\\hat{Y}$ as the matrix product of your features data X and parameters theta, in that order. \n",
    "\n",
    "Don't forget to add a 0th column of ones to X to account for the bias/offset parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SI8oTUQsdnNk"
   },
   "outputs": [],
   "source": [
    "# Define your hypothesis function according to the instructions above\n",
    "def h (theta, X) :\n",
    "    ### START CODE HERE ###\n",
    "    tempX = None\n",
    "    return None\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LW5F5uhdnNm"
   },
   "source": [
    "Define your loss function as **half** the MSE (mean squared error) between your actual and predicted Y values. \n",
    "\n",
    "Recall that the predicted Y values are a function of theta and X.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzRpZec4dnNm"
   },
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "def loss (theta, X, Y) :\n",
    "    ### START CODE HERE ###\n",
    "    return None\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCLgRXivdnNp"
   },
   "source": [
    "### Gradient of Hypothesis Function\n",
    "\n",
    "One can verify through straightforward (if somewhat tedious) multivariable calculus that the gradient of the loss function $J$ with respect to the parameters $\\theta$ is \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = - \\frac{1}{m} X^T \\cdot (Y - \\hat{Y})$$.\n",
    "\n",
    "Here, $X$ has been augmented with a bias column. \n",
    "\n",
    "Set up a function to compute this gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zYnvBa1dnNp"
   },
   "outputs": [],
   "source": [
    "def gradient (theta, X, Y) :\n",
    "    ### START CODE HERE ###\n",
    "    # Create a temporary X array with an added 0th bias column\n",
    "    tempX = None\n",
    "    # Compute the gradient according to the instructions above\n",
    "    d_theta = None\n",
    "    ### END CODE HERE ###\n",
    "    return d_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo-NxmSLdnNr"
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Set up a function to train your linear regression model with gradient descent, i.e. calculate $\\frac{\\partial J}{\\partial \\theta}$ and update $\\theta$. Recall that the general gradient descent update formula is $\\theta := \\theta - \\alpha \\frac{\\partial J}{\\partial \\theta}$. We've provided the skeleton of a stochastic gradient descent function, but you're welcome to experiment with batch and/or minibatch gradient descent. Also recall that the aforementioned gradient descent methods differ in how frequently they calculate $\\frac{\\partial J}{\\partial \\theta}$ and update $\\theta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6hbmDgkdnNs"
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent (theta, X, Y, learning_rate, max_iteration, gap) :\n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the cost as an array of zeros, one for each iteration through the dataset\n",
    "    cost = np.zeros(max_iteration)\n",
    "    # Loop over the dataset\n",
    "    for i in None:\n",
    "        # Loop over each row in the dataset\n",
    "        for j in None:\n",
    "            # Compute the gradient from the current row in X and the associated Y value\n",
    "            # Make sure that both X and Y are represented as 2D row vectors\n",
    "            d_theta = None\n",
    "            # Update theta\n",
    "            theta = None\n",
    "        # Update the cost array for the current iteration\n",
    "        cost[i] = None\n",
    "    ### END CODE HERE ###\n",
    "        if i % gap == 0 :\n",
    "            print ('iteration : ', i, ' loss : ', loss(theta, X, Y)) \n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O084UJ8UdnNt"
   },
   "outputs": [],
   "source": [
    "s_theta, s_cost = stochastic_gradient_descent(s_theta, train_X, train_Y, s_learning_rate, s_max_iteration, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6D2kqejdnNv"
   },
   "source": [
    "### Generate Predictions from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-u3iTbNdnNw"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "GD_P = None\n",
    "### END CODE HERE ###\n",
    "# Set any negative predictions to 0\n",
    "GD_P[GD_P<0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ImYOho6dnNy"
   },
   "source": [
    "### Visualize the predicted and actual test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ljk24gJ4dnNy"
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(test_Y)), test_Y,  color='black')\n",
    "plt.scatter(range(len(GD_P)), GD_P, color='blue', linewidth=3)\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# This function evaluates the R**2 statistic\n",
    "# Source: https://stackoverflow.com/questions/893657/how-do-i-calculate-r-squared-using-python-and-numpy\n",
    "def r2(Yt,Yp):\n",
    "    yhat = Yp                         \n",
    "    ybar = np.sum(Yt)/len(Yt)          \n",
    "    ssreg = np.sum((yhat-ybar)**2)   \n",
    "    sstot = np.sum((Yt - ybar)**2)    \n",
    "    results = ssreg / sstot\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"RMSE, R2 using SGD=\", MSE(test_Y,GD_P), r2(test_Y,GD_P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNFkAA7gdnN0"
   },
   "source": [
    "# Task IV: Normal Equations\n",
    "\n",
    "Since our training dataset isn't very large, let's generate predictions using the normal equations: \n",
    "\n",
    "$$W = (X^T \\cdot X)^{-1} \\cdot X^T \\cdot Y$$ \n",
    "$$\\hat{Y} = X \\cdot W$$\n",
    "\n",
    "and see how they compare to the predictions which we obtained from gradient descent.\n",
    "Ensure $$Y=[nx1], W=[dx1], X[nxd]$$ dimensions, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ez7XC2qQdnN1"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Compute the inverse of the matrix product of the transpose of X and X\n",
    "inv1 = None\n",
    "# Compute W using the first of the Normal Equations\n",
    "W1 = None\n",
    "# Compute the predicted Y values using the second of the Normal Equations\n",
    "PN1 = None\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Set any negative predictions to 0\n",
    "PN1[np.where(PN1<0)]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lu9NjgsJdnN3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(range(len(test_Y)), test_Y,  color='black')\n",
    "plt.scatter(range(len(PN1)), PN1, color='blue', linewidth=3)\n",
    "print(\"RMSE, R2 for prediction all features =\", MSE(test_Y,PN1), r2(test_Y,PN1)) #[Low error high corr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3ofJxdUdnN6"
   },
   "source": [
    "### Regularized Normal Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyIX4LaAdnN6"
   },
   "outputs": [],
   "source": [
    "print('Recall that our training features array train_X has')\n",
    "print(f'm = {train_X.shape[0]} rows and n = {train_X.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSvkrQ6EdnN8"
   },
   "source": [
    "`train_X` is thus wider than it is tall, which suggests that the regularized normal equations might perform better in generating label predictions. In this case, we modify the first of the normal equations given above to \n",
    "\n",
    "$$W = (X^T \\cdot X + \\lambda m I)^{-1} \\cdot X^T \\cdot Y$$.\n",
    "\n",
    "Here, $\\lambda$ is the regularization parameter and $m$ is the number of rows in $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Mgd1zocdnN9"
   },
   "source": [
    "### Repeat the previous parts of Task IV, but this time incorporate regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIrbJkOydnN9"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5uD0DVidnN_"
   },
   "source": [
    "# Task V: Non-linear Regression Models (GLM, DT) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yk0Be7SQdnN_"
   },
   "source": [
    "### Generalized Linear Models\n",
    "\n",
    "`sm` (our alias for `statsmodels.api`) contains a `GLM` class. Use it to instantiate a model. The relevant parameters are training labels, training features, and `ffamily`, i.e. the family of distributions to which we assume our prediction errors belong. Some potentially good choices for `ffamily` include Gaussian, Gamma, and Logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxUdD_LhdnN_"
   },
   "outputs": [],
   "source": [
    "# GLM \n",
    "import statsmodels.api as sm\n",
    "### START CODE HERE ###\n",
    "# Instantiate the GLM\n",
    "glm_gamma = None\n",
    "# Train the GLM\n",
    "glm_results = None\n",
    "### END CODE HERE ###\n",
    "print(glm_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IAchWHodnOB"
   },
   "source": [
    "### Generate predictions from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLTsITX7dnOC"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "G_P = None\n",
    "### END CODE HERE ###\n",
    "# Set any negative predictions to 0\n",
    "G_P[G_P<0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgXIT-JpdnOE"
   },
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(range(len(test_Y)), test_Y,  color='black')\n",
    "plt.scatter(range(len(G_P)), G_P, color='blue', linewidth=3)\n",
    "print(\"RMSE, R2 for GLM=\", MSE(test_Y,G_P), r2(test_Y,G_P)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vld0rNUcdnOG"
   },
   "source": [
    "### Random Forest Regression\n",
    "\n",
    "Use the `RandomForestRegressor` from `sklearn.ensemble` to generate predictions. The relevant parameters are the `max_depth` of the trees and the `random_state`, to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbxRUm3UdnOG"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate the random forest regression model\n",
    "regr = None\n",
    "# Train the model\n",
    "None\n",
    "# Generate predictions from the test data\n",
    "pred_rf = None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWVeR1PvdnOI"
   },
   "outputs": [],
   "source": [
    "# Set any negative predictions to 0\n",
    "pred_rf[np.where(pred_rf<0)]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSZX9t1MdnOK"
   },
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(range(len(test_Y)), test_Y,  color='black')\n",
    "plt.scatter(range(len(pred_rf)), pred_rf, color='blue', linewidth=3)\n",
    "print(\"RMSE, R2 for Decision Trees=\", MSE(test_Y,pred_rf), r2(test_Y,pred_rf)) #[Low error high corr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxx1JrikdnON"
   },
   "source": [
    "## Populate the table below with the results of your experiments above. Which models performed best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44MPnft4dnON"
   },
   "source": [
    "## Results\n",
    "---------------------------------------------------------------------------\n",
    "Method                                          |  RMSE             | R2               |\n",
    "-------------------------------------------------------------------------------------\n",
    "1. Gradient Descent\n",
    "2. Normal Equations\n",
    "3. Regularized Normal Equations\n",
    "4. GLM\n",
    "5. Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_HoMbxTdnOO"
   },
   "source": [
    "## Finally, pick some car types for which your models over- and under-predicted sales. Attempt to determine the root causes. Write your findings below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WddJkKQgdnOO"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Lx-wS125dnNk",
    "PCLgRXivdnNp",
    "fo-NxmSLdnNr",
    "B6D2kqejdnNv",
    "2ImYOho6dnNy",
    "i3ofJxdUdnN6"
   ],
   "name": "Car_sales_predictions_v1 (1).ipynb",
   "provenance": [
    {
     "file_id": "1JtLnLB6o4eGEPMFrkUAKjikvZYxMQjKb",
     "timestamp": 1603345491277
    },
    {
     "file_id": "19YEpf20f30cjnoCzCIGwplKK0slRMKrD",
     "timestamp": 1603345211592
    },
    {
     "file_id": "1A5aKkAXYKm5ju0TNG-1yU44-hnWEhh__",
     "timestamp": 1603344980309
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
